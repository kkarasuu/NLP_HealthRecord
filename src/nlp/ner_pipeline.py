from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import os


def load_text(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


def extract_entities(text: str):
    model_name = "DeepPavlov/rubert-base-cased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForTokenClassification.from_pretrained(model_name)

    ner = pipeline("ner", model=model, tokenizer=tokenizer, grouped_entities=True)
    results = ner(text)

    entities = [(r["word"], r["entity_group"]) for r in results]
    return entities


if __name__ == "__main__":
    file_path = "data/processed_texts/sample_medical_record_10_pages.txt"

    if not os.path.exists(file_path):
        print("‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω:", file_path)
        exit()

    print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Ñ–∞–π–ª: {file_path}")
    text = load_text(file_path)

    print("üîç –ò–∑–≤–ª–µ–∫–∞—é—Ç—Å—è —Å—É—â–Ω–æ—Å—Ç–∏...")
    entities = extract_entities(text[:1000])

    print("\nüß† –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:")
    for word, label in entities:
        print(f"{word} ‚Üí {label}")
