import argparse
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F

# === –ê—Ä–≥—É–º–µ–Ω—Ç—ã ===
parser = argparse.ArgumentParser()
group = parser.add_mutually_exclusive_group(required=True)
group.add_argument("--text", type=str, help="–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç")
group.add_argument("--file", type=str, help="–ü—É—Ç—å –∫ .txt —Ñ–∞–π–ª—É (–∞–Ω–∞–ª–∏–∑ –ø–æ —Å—Ç—Ä–æ–∫–∞–º)")
args = parser.parse_args()

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ===
model_path = "models/classifier"
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)
model.eval()

# === –ü–æ–ª—É—á–∞–µ–º id2label –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
id2label = model.config.id2label
if isinstance(id2label, dict):
    id2label = {int(k): v for k, v in id2label.items()}

# === –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
def classify(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = F.softmax(outputs.logits, dim=-1)
        sorted_indices = torch.argsort(probs[0], descending=True)
        return [(id2label[i.item()], probs[0][i].item()) for i in sorted_indices[:3]]

# === –†–µ–∂–∏–º: –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç
if args.text:
    print(f"\nüìÑ –¢–µ–∫—Å—Ç: {args.text}")
    for label, score in classify(args.text):
        print(f"‚Üí {label:<15} {score * 100:.1f}%")

# === –†–µ–∂–∏–º: –ø–æ —Ñ–∞–π–ª—É (—Å—Ç—Ä–æ–∫–∞ –∑–∞ —Å—Ç—Ä–æ–∫–æ–π)
if args.file:
    with open(args.file, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if len(line.strip()) > 30]
    for i, line in enumerate(lines, 1):
        print(f"\nüßæ –§—Ä–∞–≥–º–µ–Ω—Ç {i}: {line}")
        for label, score in classify(line):
            print(f"‚Üí {label:<15} {score * 100:.1f}%")
