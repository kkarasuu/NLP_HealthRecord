import torch
import json
import fitz  # PyMuPDF
import argparse
import os
import nltk
import nltk.data
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, pipeline
from nltk.tokenize.punkt import PunktSentenceTokenizer
import pickle

nltk.download('punkt_tab')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# === –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ ===
parser = argparse.ArgumentParser(description="Medical text inference pipeline")
parser.add_argument("--pdf", type=str, required=True, help="Path to input PDF file")
parser.add_argument("--specialty", type=str, required=True, help="Target medical specialty")
parser.add_argument("--output", type=str, default="output.json", help="Path to save the result JSON")
args = parser.parse_args()

# === –ó–∞–≥—Ä—É–∑–∫–∞ PDF ===
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join(page.get_text() for page in doc)

# === –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º
spec_model_path = "models/specialty_filter"
cat_model_path = "models/category_classifier"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
spec_model = AutoModelForSequenceClassification.from_pretrained(spec_model_path, local_files_only=True).to(device)
spec_tokenizer = AutoTokenizer.from_pretrained(spec_model_path, local_files_only=True)
cat_model = AutoModelForTokenClassification.from_pretrained(cat_model_path, local_files_only=True).to(device)
cat_tokenizer = AutoTokenizer.from_pretrained(cat_model_path, local_files_only=True)

ner = pipeline("ner", model=cat_model, tokenizer=cat_tokenizer, aggregation_strategy="simple", device=0 if torch.cuda.is_available() else -1)

# === –ó–∞–≥—Ä—É–∑–∫–∞ Punkt Sentence Tokenizer
try:
    tokenizer_path = "/usr/share/nltk_data/tokenizers/punkt/english.pickle"
    with open(tokenizer_path, "rb") as f:
        tokenizer = pickle.load(f)
except Exception as e:
    print("[ERROR] Failed to load Punkt tokenizer:", e)
    tokenizer = PunktSentenceTokenizer()

# === –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏
def is_relevant(sentence, target_class, label2id):
    inputs = spec_tokenizer(sentence, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        logits = spec_model(**inputs).logits
        predicted = torch.argmax(logits, dim=1).item()
    return predicted == label2id[target_class]

# === –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
def run_pipeline(pdf_path, target_specialty):
    print("=== üîç –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ ===")
    text = extract_text_from_pdf(pdf_path)
    print(f"[DEBUG] –ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞:\n{text[:500]}")

    sentences = tokenizer.tokenize(text)
    print(f"[DEBUG] –ö–æ–ª-–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(sentences)}")

    with open(os.path.join(spec_model_path, "config.json")) as f:
        config = json.load(f)
        label2id = config["label2id"]
    print(f"[DEBUG] –ú–µ—Ç–∫–∏: {list(label2id.keys())}")

    relevant = [s for s in sentences if is_relevant(s, target_specialty, label2id)]
    print(f"[DEBUG] –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(relevant)}")

    result = {
        "specialty": target_specialty,
        "diagnoses": [],
        "complaints": [],
        "procedures": [],
        "treatments": [],
        "test_results": []
    }

    label_map = {
        "LABEL_1": "Diagnosis",
        "LABEL_2": "Symptom",
        "LABEL_3": "Treatment",
        "LABEL_4": "Procedure",
        "LABEL_5": "TestResult"
    }

    mapping = {
        "Diagnosis": "diagnoses",
        "Symptom": "complaints",
        "Treatment": "treatments",
        "Procedure": "procedures",
        "TestResult": "test_results"
    }

    bad_phrases = {
        "section", "sections", "conclusion", "personal", "history", "name", "date", "address",
        "signature", "bp", "bpm", "doctor", "physician", "institution"
    }

    for sentence in relevant:
        print(f"[DEBUG] ‚Üí NER –Ω–∞: {sentence}")
        entities = ner(sentence)
        print(f"[DEBUG] ‚Üê –°—É—â–Ω–æ—Å—Ç–∏: {entities}")

        for ent in entities:
            group = ent["entity_group"]
            label = label_map.get(group)
            if not label:
                continue

            raw = ent["word"].strip()
            if len(raw) < 4 or any(bad in raw.lower() for bad in bad_phrases):
                continue

            tokens = [t.strip() for part in raw.split(" and ") for t in part.replace(";", ",").split(",")]
            for token in tokens:
                if len(token) < 4 or any(bad in token.lower() for bad in bad_phrases):
                    continue
                if token not in result[mapping[label]]:
                    result[mapping[label]].append(token)

    return result

# === –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if __name__ == "__main__":
    output = run_pipeline(args.pdf, args.specialty)

    print("\n=== üß© –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(json.dumps(output, indent=2, ensure_ascii=False))

    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {args.output}")
